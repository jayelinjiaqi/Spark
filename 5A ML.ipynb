{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6ffed39",
   "metadata": {},
   "source": [
    "# Pipelines\n",
    "\n",
    "SparkML uses pipeline concept with Transformers (eg. VectorAssembler) and Estimators (eg. LinearRegression, LogisticRegression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07603d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/07/27 13:58:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"5A-ML\") \\\n",
    "    .master(\"yarn\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33916d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/27 13:58:43 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+----+----+-------+----------+--------+---------+----+---------+----+--------+------+----------+----------+----+---------+--------+---------+-------+----------------+----------+----------+--------------+-----------+-------+----------+--------------------+-------------------+--------------------+------------+------------+\n",
      "|north|south|east|west|central|  psi_date|psi_hour|     name|temp|feelslike| dew|humidity|precip|precipprob|preciptype|snow|snowdepth|windgust|windspeed|winddir|sealevelpressure|cloudcover|visibility|solarradiation|solarenergy|uvindex|severerisk|          conditions|               icon|            stations|weather_date|weather_hour|\n",
      "+-----+-----+----+----+-------+----------+--------+---------+----+---------+----+--------+------+----------+----------+----+---------+--------+---------+-------+----------------+----------+----------+--------------+-----------+-------+----------+--------------------+-------------------+--------------------+------------+------------+\n",
      "|   29|   27|  32|  17|     29|2022-01-01|       0|Singapore|25.0|     25.0|24.0|   94.03|   0.0|         0|      NULL|   0|        0|    25.9|      5.2|   32.0|          1012.7|      63.2|       8.7|           0.0|        0.0|      0|      NULL|    Partially cloudy|partly-cloudy-night|48698099999,48679...|  2022-01-01|           0|\n",
      "|   30|   25|  32|  18|     29|2022-01-01|       1|Singapore|24.3|     24.3|23.9|   97.64| 3.473|       100|      rain|   0|        0|    25.6|      8.5|  137.0|          1012.1|      88.0|       9.6|           0.0|        0.0|      0|      NULL|Rain, Partially c...|               rain|48698099999,48679...|  2022-01-01|           1|\n",
      "|   30|   24|  31|  19|     30|2022-01-01|       2|Singapore|23.9|     23.9|22.4|   91.01| 4.341|       100|      rain|   0|        0|    27.7|     13.9|   91.0|          1011.1|      90.3|       8.1|           0.0|        0.0|      0|      NULL|      Rain, Overcast|               rain|48698099999,48679...|  2022-01-01|           2|\n",
      "|   30|   23|  30|  19|     31|2022-01-01|       3|Singapore|24.0|     24.0|23.0|   94.06| 0.868|       100|      rain|   0|        0|    21.2|      6.7|   63.0|          1011.0|      88.0|       9.5|           0.0|        0.0|      0|      NULL|Rain, Partially c...|               rain|48698099999,48679...|  2022-01-01|           3|\n",
      "|   31|   23|  29|  20|     30|2022-01-01|       4|Singapore|23.9|     23.9|22.9|   94.24| 0.868|       100|      rain|   0|        0|    20.5|      3.0|  357.0|          1010.0|      91.8|       9.1|           0.0|        0.0|      0|      NULL|      Rain, Overcast|               rain|48698099999,48679...|  2022-01-01|           4|\n",
      "+-----+-----+----+----+-------+----------+--------+---------+----+---------+----+--------+------+----------+----------+----+---------+--------+---------+-------+----------------+----------+----------+--------------+-----------+-------+----------+--------------------+-------------------+--------------------+------------+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- north: integer (nullable = true)\n",
      " |-- south: integer (nullable = true)\n",
      " |-- east: integer (nullable = true)\n",
      " |-- west: integer (nullable = true)\n",
      " |-- central: integer (nullable = true)\n",
      " |-- psi_date: date (nullable = true)\n",
      " |-- psi_hour: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- temp: double (nullable = true)\n",
      " |-- feelslike: double (nullable = true)\n",
      " |-- dew: double (nullable = true)\n",
      " |-- humidity: double (nullable = true)\n",
      " |-- precip: double (nullable = true)\n",
      " |-- precipprob: integer (nullable = true)\n",
      " |-- preciptype: string (nullable = true)\n",
      " |-- snow: integer (nullable = true)\n",
      " |-- snowdepth: integer (nullable = true)\n",
      " |-- windgust: double (nullable = true)\n",
      " |-- windspeed: double (nullable = true)\n",
      " |-- winddir: double (nullable = true)\n",
      " |-- sealevelpressure: double (nullable = true)\n",
      " |-- cloudcover: double (nullable = true)\n",
      " |-- visibility: double (nullable = true)\n",
      " |-- solarradiation: double (nullable = true)\n",
      " |-- solarenergy: double (nullable = true)\n",
      " |-- uvindex: integer (nullable = true)\n",
      " |-- severerisk: integer (nullable = true)\n",
      " |-- conditions: string (nullable = true)\n",
      " |-- icon: string (nullable = true)\n",
      " |-- stations: string (nullable = true)\n",
      " |-- weather_date: date (nullable = true)\n",
      " |-- weather_hour: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load merged PSI-Weather data (from HDFS)\n",
    "\n",
    "merged_path = \"hdfs:///data/psi_weather_merged.parquet\"\n",
    "df_merged = spark.read.parquet(merged_path)\n",
    "df_merged.show(5)\n",
    "df_merged.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89611dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create or transform relevant columns\n",
    "# a. PSI label: avg of (north, south, east, west, central) -> psi_avg\n",
    "# b. Classification label (1 if psi_avg >= 10, otherwise 0)\n",
    "# c. Numeric weather features: temp, humidity, windspeed, precip\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "df_features = df_merged.withColumn(\n",
    "    \"psi_avg\",\n",
    "    (F.col(\"north\") + F.col(\"south\") + F.col(\"east\") + F.col(\"west\") + F.col(\"central\"))/5.0\n",
    ")\n",
    "\n",
    "# Classification label: unhealthy if psi_avg >= 101\n",
    "df_features = df_features.withColumn(\n",
    "    \"label_class\",\n",
    "    F.when(F.col(\"psi_avg\") >= 101, 1).otherwise(0)\n",
    ")\n",
    "\n",
    "\n",
    "# 3.\tCreate or transform relevant columns:\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"temp\", \"humidity\", \"windspeed\", \"precip\"],  # adjust as needed\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "df_final = assembler.transform(df_features)\n",
    "# Now df_final has columns: psi_avg, label_class, features\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046481a8",
   "metadata": {},
   "source": [
    "# Linear Regression (Predict PSI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3557c250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into training/test\n",
    "\n",
    "train_df, test_df = df_final.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34abd60c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/27 14:02:03 WARN Instrumentation: [739dccd6] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/07/27 14:02:05 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "25/07/27 14:02:05 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# train linear regression model\n",
    "\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"psi_avg\")\n",
    "lr_model = lr.fit(train_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04b8a49f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------------------+\n",
      "|psi_avg|            features|        prediction|\n",
      "+-------+--------------------+------------------+\n",
      "|   15.8|[24.6,90.74,6.7,0...|37.303399321754796|\n",
      "|   15.2|[24.0,94.07,3.8,0.0]| 36.86814759898148|\n",
      "|   15.2|[23.4,97.42,4.2,0...|36.629502657292875|\n",
      "|   18.4|[24.0,93.88,6.4,0...| 36.90886236654545|\n",
      "|   19.0|[29.6,63.54,8.6,0.0]|  39.7418847383947|\n",
      "+-------+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# predict on test set\n",
    "\n",
    "predictions_lr = lr_model.transform(test_df)\n",
    "predictions_lr.select(\"psi_avg\", \"features\", \"prediction\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ab60544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression RMSE: 10.381760785172588\n"
     ]
    }
   ],
   "source": [
    "# evaluate with RegressionEvaluator\n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"psi_avg\", \n",
    "    predictionCol=\"prediction\", \n",
    "    metricName=\"rmse\"  # or \"mae\", \"r2\"\n",
    ")\n",
    "rmse = evaluator.evaluate(predictions_lr)\n",
    "print(\"Linear Regression RMSE:\", rmse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307f1d1e",
   "metadata": {},
   "source": [
    "# K-Means Clustering (Group Similar Conditions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42af8830",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# import k-means algo\n",
    "\n",
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "kmeans = KMeans(featuresCol=\"features\", k=3, seed=42)\n",
    "kmeans_model = kmeans.fit(df_final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0ef6be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+----+----+-------+----------+--------+---------+----+---------+----+--------+------+----------+----------+----+---------+--------+---------+-------+----------------+----------+----------+--------------+-----------+-------+----------+--------------------+-------------------+--------------------+------------+------------+-------+-----------+--------------------+----------+\n",
      "|north|south|east|west|central|  psi_date|psi_hour|     name|temp|feelslike| dew|humidity|precip|precipprob|preciptype|snow|snowdepth|windgust|windspeed|winddir|sealevelpressure|cloudcover|visibility|solarradiation|solarenergy|uvindex|severerisk|          conditions|               icon|            stations|weather_date|weather_hour|psi_avg|label_class|            features|prediction|\n",
      "+-----+-----+----+----+-------+----------+--------+---------+----+---------+----+--------+------+----------+----------+----+---------+--------+---------+-------+----------------+----------+----------+--------------+-----------+-------+----------+--------------------+-------------------+--------------------+------------+------------+-------+-----------+--------------------+----------+\n",
      "|   29|   27|  32|  17|     29|2022-01-01|       0|Singapore|25.0|     25.0|24.0|   94.03|   0.0|         0|      NULL|   0|        0|    25.9|      5.2|   32.0|          1012.7|      63.2|       8.7|           0.0|        0.0|      0|      NULL|    Partially cloudy|partly-cloudy-night|48698099999,48679...|  2022-01-01|           0|   26.8|          0|[25.0,94.03,5.2,0.0]|         0|\n",
      "|   30|   25|  32|  18|     29|2022-01-01|       1|Singapore|24.3|     24.3|23.9|   97.64| 3.473|       100|      rain|   0|        0|    25.6|      8.5|  137.0|          1012.1|      88.0|       9.6|           0.0|        0.0|      0|      NULL|Rain, Partially c...|               rain|48698099999,48679...|  2022-01-01|           1|   26.8|          0|[24.3,97.64,8.5,3...|         0|\n",
      "|   30|   24|  31|  19|     30|2022-01-01|       2|Singapore|23.9|     23.9|22.4|   91.01| 4.341|       100|      rain|   0|        0|    27.7|     13.9|   91.0|          1011.1|      90.3|       8.1|           0.0|        0.0|      0|      NULL|      Rain, Overcast|               rain|48698099999,48679...|  2022-01-01|           2|   26.8|          0|[23.9,91.01,13.9,...|         0|\n",
      "|   30|   23|  30|  19|     31|2022-01-01|       3|Singapore|24.0|     24.0|23.0|   94.06| 0.868|       100|      rain|   0|        0|    21.2|      6.7|   63.0|          1011.0|      88.0|       9.5|           0.0|        0.0|      0|      NULL|Rain, Partially c...|               rain|48698099999,48679...|  2022-01-01|           3|   26.6|          0|[24.0,94.06,6.7,0...|         0|\n",
      "|   31|   23|  29|  20|     30|2022-01-01|       4|Singapore|23.9|     23.9|22.9|   94.24| 0.868|       100|      rain|   0|        0|    20.5|      3.0|  357.0|          1010.0|      91.8|       9.1|           0.0|        0.0|      0|      NULL|      Rain, Overcast|               rain|48698099999,48679...|  2022-01-01|           4|   26.6|          0|[23.9,94.24,3.0,0...|         0|\n",
      "+-----+-----+----+----+-------+----------+--------+---------+----+---------+----+--------+------+----------+----------+----+---------+--------+---------+-------+----------------+----------+----------+--------------+-----------+-------+----------+--------------------+-------------------+--------------------+------------+------------+-------+-----------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get cluster predictions\n",
    "\n",
    "df_clusters = kmeans_model.transform(df_final)\n",
    "df_clusters.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "824ef5fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Means Silhouette score: 0.602169149636199\n"
     ]
    }
   ],
   "source": [
    "# evaluate using ClusteringEvaluator\n",
    "\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "evaluator_cluster = ClusteringEvaluator(featuresCol=\"features\", predictionCol=\"prediction\")\n",
    "silhouette = evaluator_cluster.evaluate(df_clusters)\n",
    "print(\"K-Means Silhouette score:\", silhouette)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ca4f7ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0 center: [26.03534483 89.20528271  5.65360041  0.49088628]\n",
      "Cluster 1 center: [3.10574622e+01 6.23968774e+01 1.31865884e+01 1.74682773e-02]\n",
      "Cluster 2 center: [2.81359948e+01 7.75427202e+01 9.17843866e+00 7.63361888e-02]\n"
     ]
    }
   ],
   "source": [
    "# Analyze cluster centers\n",
    "\n",
    "centers = kmeans_model.clusterCenters()\n",
    "for i, center in enumerate(centers):\n",
    "    print(f\"Cluster {i} center:\", center)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a788e15",
   "metadata": {},
   "source": [
    "# Logistic Regression (Healthy vs Unhealthy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc2571f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train/test\n",
    "\n",
    "train_cls, test_cls = df_final.randomSplit([0.8, 0.2], seed=99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab9f831c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/27 14:04:35 WARN Instrumentation: [35df831c] All labels are the same value and fitIntercept=true, so the coefficients will be zeros. Training is not needed.\n"
     ]
    }
   ],
   "source": [
    "# train LR \n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr_cls = LogisticRegression(\n",
    "    featuresCol=\"features\", \n",
    "    labelCol=\"label_class\", \n",
    "    probabilityCol=\"probability\"\n",
    ")\n",
    "lr_cls_model = lr_cls.fit(train_cls)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dda6e178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+----------+\n",
      "|label_class|probability|prediction|\n",
      "+-----------+-----------+----------+\n",
      "|          0|  [1.0,0.0]|       0.0|\n",
      "|          0|  [1.0,0.0]|       0.0|\n",
      "|          0|  [1.0,0.0]|       0.0|\n",
      "|          0|  [1.0,0.0]|       0.0|\n",
      "|          0|  [1.0,0.0]|       0.0|\n",
      "+-----------+-----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# predict\n",
    "\n",
    "predictions_cls = lr_cls_model.transform(test_cls)\n",
    "predictions_cls.select(\"label_class\", \"probability\", \"prediction\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a2c6759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# evaluate with MultiClassificationEvaluator\n",
    "\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator_cls = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label_class\", \n",
    "    predictionCol=\"prediction\", \n",
    "    metricName=\"accuracy\"\n",
    ")\n",
    "accuracy = evaluator_cls.evaluate(predictions_cls)\n",
    "print(\"Logistic Regression Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f930e9b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[INVALID_EXTRACT_BASE_FIELD_TYPE] Can't extract a value from \"probability\". Need a complex type [STRUCT, ARRAY, MAP] but got \"STRUCT<type: TINYINT, size: INT, indices: ARRAY<INT>, values: ARRAY<DOUBLE>>\".",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m functions \u001b[38;5;28;01mas\u001b[39;00m F\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# For binary, we might see if the model is uncertain by threshold\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m df_top2 \u001b[38;5;241m=\u001b[39m \u001b[43mpredictions_cls\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpredicted_class\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprobability\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43motherwise\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# \"Optimization\" might mean focusing on borderline cases or adjusting the decision threshold.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m df_top2\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m5\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py:5176\u001b[0m, in \u001b[0;36mDataFrame.withColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   5171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, Column):\n\u001b[1;32m   5172\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m   5173\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_COLUMN\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   5174\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcol\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(col)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m   5175\u001b[0m     )\n\u001b[0;32m-> 5176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolName\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jc\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [INVALID_EXTRACT_BASE_FIELD_TYPE] Can't extract a value from \"probability\". Need a complex type [STRUCT, ARRAY, MAP] but got \"STRUCT<type: TINYINT, size: INT, indices: ARRAY<INT>, values: ARRAY<DOUBLE>>\"."
     ]
    }
   ],
   "source": [
    "# pick 2 highest probability (if multi-class)\n",
    "# for multi-class, examine probability vector, \n",
    "# pick 2 top classes by probability, do further analusis or \"optimization\"ArithmeticError\n",
    "\n",
    "# Example for binary classification: probability = [prob_of_class_0, prob_of_class_1]\n",
    "# If it was multi-class, probability could be length > 2, we pick top 2 indices.\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# For binary, we might see if the model is uncertain by threshold\n",
    "df_top2 = predictions_cls.withColumn(\n",
    "    \"predicted_class\",\n",
    "    F.when(F.col(\"probability\")[1] > 0.5, 1).otherwise(0)\n",
    ")\n",
    "# \"Optimization\" might mean focusing on borderline cases or adjusting the decision threshold.\n",
    "df_top2.show(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f64a02fa",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[INVALID_EXTRACT_BASE_FIELD_TYPE] Can't extract a value from \"probability\". Need a complex type [STRUCT, ARRAY, MAP] but got \"STRUCT<type: TINYINT, size: INT, indices: ARRAY<INT>, values: ARRAY<DOUBLE>>\".",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df2 \u001b[38;5;241m=\u001b[39m \u001b[43mpredictions_cls\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprob_class1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprobability\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m df2 \u001b[38;5;241m=\u001b[39m df2\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredicted\u001b[39m\u001b[38;5;124m\"\u001b[39m, F\u001b[38;5;241m.\u001b[39mwhen(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprob_class1\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39motherwise(\u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py:5176\u001b[0m, in \u001b[0;36mDataFrame.withColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   5171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, Column):\n\u001b[1;32m   5172\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m   5173\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_COLUMN\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   5174\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcol\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(col)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m   5175\u001b[0m     )\n\u001b[0;32m-> 5176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolName\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jc\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [INVALID_EXTRACT_BASE_FIELD_TYPE] Can't extract a value from \"probability\". Need a complex type [STRUCT, ARRAY, MAP] but got \"STRUCT<type: TINYINT, size: INT, indices: ARRAY<INT>, values: ARRAY<DOUBLE>>\"."
     ]
    }
   ],
   "source": [
    "df2 = predictions_cls.withColumn(\"prob_class1\", col(\"probability\").values[1])\n",
    "df2 = df2.withColumn(\"predicted\", F.when(col(\"prob_class1\") > 0.5, 1).otherwise(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d4c5647c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 70:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+----+----+-------+----------+--------+---------+----+---------+----+--------+------+----------+----------+----+---------+--------+---------+-------+----------------+----------+----------+--------------+-----------+-------+----------+--------------------+-------------------+--------------------+------------+------------+-------+-----------+--------------------+--------------------+-----------+----------+-----------+---------+\n",
      "|north|south|east|west|central|  psi_date|psi_hour|     name|temp|feelslike| dew|humidity|precip|precipprob|preciptype|snow|snowdepth|windgust|windspeed|winddir|sealevelpressure|cloudcover|visibility|solarradiation|solarenergy|uvindex|severerisk|          conditions|               icon|            stations|weather_date|weather_hour|psi_avg|label_class|            features|       rawPrediction|probability|prediction|prob_class1|predicted|\n",
      "+-----+-----+----+----+-------+----------+--------+---------+----+---------+----+--------+------+----------+----------+----+---------+--------+---------+-------+----------------+----------+----------+--------------+-----------+-------+----------+--------------------+-------------------+--------------------+------------+------------+-------+-----------+--------------------+--------------------+-----------+----------+-----------+---------+\n",
      "|   14|   17|  20|  16|     13|2023-01-30|       0|Singapore|24.0|     24.0|22.4|   90.38|   0.0|         0|      NULL|   0|        0|    33.5|      5.6|  335.0|          1010.1|      63.2|      10.0|           0.0|        0.0|      0|      NULL|    Partially cloudy|partly-cloudy-night|48698099999,48679...|  2023-01-30|           0|   16.0|          0|[24.0,90.38,5.6,0.0]|[Infinity,-Infinity]|  [1.0,0.0]|       0.0|        0.0|        0|\n",
      "|   15|   15|  17|  15|     14|2023-01-25|       8|Singapore|24.0|     24.0|23.0|   94.07|   0.0|         0|      NULL|   0|        0|    32.4|      3.8|   39.0|          1009.0|      90.3|       9.1|          10.1|        0.0|      0|      NULL|            Overcast|             cloudy|48698099999,48679...|  2023-01-25|           8|   15.2|          0|[24.0,94.07,3.8,0.0]|[Infinity,-Infinity]|  [1.0,0.0]|       0.0|        0.0|        0|\n",
      "|   15|   15|  18|  15|     13|2023-01-25|       7|Singapore|23.4|     23.4|23.0|   97.42| 0.174|       100|      rain|   0|        0|    32.4|      4.2|  293.0|          1008.0|      89.1|       8.9|           0.0|        0.0|      0|      NULL|Rain, Partially c...|               rain|48698099999,48679...|  2023-01-25|           7|   15.2|          0|[23.4,97.42,4.2,0...|[Infinity,-Infinity]|  [1.0,0.0]|       0.0|        0.0|        0|\n",
      "|   15|   15|  18|  15|     13|2023-01-26|       7|Singapore|23.1|     23.1|22.4|   95.83|   0.0|         0|      NULL|   0|        0|    26.6|      2.3|  218.0|          1009.7|     100.0|       7.1|           0.0|        0.0|      0|      NULL|            Overcast|             cloudy|48698099999,48679...|  2023-01-26|           7|   15.2|          0|[23.1,95.83,2.3,0.0]|[Infinity,-Infinity]|  [1.0,0.0]|       0.0|        0.0|        0|\n",
      "|   15|   18|  21|  18|     14|2023-01-29|      20|Singapore|25.0|     25.0|22.3|   85.22|   0.0|         0|      NULL|   0|        0|    37.4|      9.8|  356.0|          1009.0|      89.2|      10.2|           1.6|        0.0|      0|      NULL|    Partially cloudy|partly-cloudy-night|48698099999,48679...|  2023-01-29|          20|   17.2|          0|[25.0,85.22,9.8,0.0]|[Infinity,-Infinity]|  [1.0,0.0]|       0.0|        0.0|        0|\n",
      "+-----+-----+----+----+-------+----------+--------+---------+----+---------+----+--------+------+----------+----------+----+---------+--------+---------+-------+----------------+----------+----------+--------------+-----------+-------+----------+--------------------+-------------------+--------------------+------------+------------+-------+-----------+--------------------+--------------------+-----------+----------+-----------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf, col, when\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# UDF to extract the probability for class 1 from the vector\n",
    "def extract_prob_class1(v):\n",
    "    return float(v.values[1]) if v is not None else None\n",
    "\n",
    "extract_prob1_udf = udf(extract_prob_class1, DoubleType())\n",
    "\n",
    "df2 = predictions_cls \\\n",
    "    .withColumn(\"prob_class1\", extract_prob1_udf(col(\"probability\"))) \\\n",
    "    .withColumn(\"predicted\", when(col(\"prob_class1\") > 0.5, 1).otherwise(0))\n",
    "\n",
    "df2.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cae7b1",
   "metadata": {},
   "source": [
    "# Comparing & Plotting Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f50f1f8c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rmse_linreg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# create pandas df for metrics\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      5\u001b[0m results \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLinearReg\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKMeans\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLogReg\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDeepLearning\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScore\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[43mrmse_linreg\u001b[49m, silhouette_kmeans, accuracy_logreg, accuracy_dl]\n\u001b[1;32m      8\u001b[0m }\n\u001b[1;32m      9\u001b[0m pdf_results \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(results)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(pdf_results)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rmse_linreg' is not defined"
     ]
    }
   ],
   "source": [
    "# create pandas df for metrics\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "results = {\n",
    "    \"Model\": [\"LinearReg\", \"KMeans\", \"LogReg\", \"DeepLearning\"],\n",
    "    \"Score\": [rmse_linreg, silhouette_kmeans, accuracy_logreg, accuracy_dl]\n",
    "}\n",
    "pdf_results = pd.DataFrame(results)\n",
    "print(pdf_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a029ed7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
